---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 1.0.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Machine learning from scratch - Neural Network

This Jupyter notebook implements a neural network from scratch using `numpy`.

```{python}
import numpy as np
from sklearn.datasets import load_digits
```

```{python}
np.random.seed(0)

class Layer(object):
    def init(self):
        pass
    
    def forward(x):
        pass
    
    def grad(target):
        pass

class Linear(Layer):
    def __init__(self, n_input, n_output):
        self.w = np.random.random((n_input, n_output)) * 0.01
    
    def forward(self, x):
        # x: (N_samples, n_input), w: (n_input, n_output)
        self.input = x
        return x @ self.w
    
    # calculate the gradient of this layer w.r.t. to its weights
    def backward(self, grad_after):
        m = self.input.shape[0]
        self.grad_w = self.input.T @ grad_after / m
        return grad_after @ self.w.T
    
    def update(self, alpha):
        self.w -= alpha * self.grad_w

n_classes = 10
x, y = load_digits(n_class=n_classes, return_X_y=True)
x = x / np.max(x)
y_oh = np.eye(n_classes)[y]

n_samples, n_features = x.shape

print(x.shape, y.shape, n_samples, n_features)

l0 = Linear(n_features, 30)
l1 = Linear(30, 20)
l2 = Linear(20, n_classes)

def cross_entropy_loss(y_hat, y_oh):
    return np.mean(-(y_oh * np.log(y_hat) + (1 - y_oh) * np.log(1 - y_hat)))

# the gradient of the above function, w.r.t. to y_hat!
def cross_entropy_grad(y_hat, y_oh):
    return -(y_oh / y_hat + (1-y_oh) / (1-y_hat))

def accuracy(y_hat, y):
    y_pred = np.argmax(y_hat, axis=1)
    return np.mean(y_pred == y)

alpha = 0.001
for i in range(1, 1000 + 1):
    a = l0.forward(x)
    a = l1.forward(a)
    y_hat = l2.forward(a)
    
    if i % 50 == 0:
        loss = cross_entropy_loss(y_hat, y_oh)
        acc = accuracy(y_hat, y)
        print("Round %d, accuracy: %.2f, loss: %4.4f" % (i, acc, loss))
    
    # note: we don't actually need the loss itself.
    # only its gradient!
    loss_grad = cross_entropy_grad(y_hat, y_oh)
    
    da = l2.backward(loss_grad)
    da = l1.backward(da)
    l0.backward(da)
    
    l0.update(alpha)
    l1.update(alpha)
    l2.update(alpha)
```

```{python}

```

```{python}

```

```{python}

```
